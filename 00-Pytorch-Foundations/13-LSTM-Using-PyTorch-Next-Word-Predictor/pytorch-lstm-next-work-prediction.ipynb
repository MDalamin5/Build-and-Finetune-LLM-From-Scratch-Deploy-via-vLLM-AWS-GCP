{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a196a4-37ed-4c46-8d78-5b780a763c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/md-al-amin/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /home/md-al-amin/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in /home/md-al-amin/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/md-al-amin/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /home/md-al-amin/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7afdab85-a156-4cf1-a0ca-63be46756611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e5157f-3768-4da7-8d10-38121657ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_1 = \"\"\"hey are u free later\n",
    "yeah i think so why\n",
    "wanna catch a movie\n",
    "sure which one\n",
    "the new marvel one\n",
    "heard its good\n",
    "lets do it what time\n",
    "maybe 7pm does that work\n",
    "perfect see u then\n",
    "don't be late this time\n",
    "i wont i promise lol\n",
    "did u finish the assignment\n",
    "no im struggling with the math part\n",
    "same its so hard\n",
    "we should study together\n",
    "yeah come over around 4\n",
    "ill bring snacks\n",
    "bring chips and salsa\n",
    "deal see ya\n",
    "hey mom can u pick me up\n",
    "im at the station\n",
    "sure give me 10 mins\n",
    "thanks love u\n",
    "love u too sweetie\n",
    "where did u put the remote\n",
    "check under the sofa\n",
    "found it thanks\n",
    "turn on the news\n",
    "no i wanna watch sports\n",
    "we always watch sports\n",
    "fine u choose\n",
    "whats for dinner\n",
    "leftover pasta\n",
    "again seriously\n",
    "if u dont like it cook something\n",
    "fine ill order pizza\n",
    "get pepperoni\n",
    "okay\n",
    "hey boss i might be late\n",
    "traffic is terrible today\n",
    "ok thanks for letting me know\n",
    "when will u be in\n",
    "around 930 hopefully\n",
    "drive safe\n",
    "meeting moved to 10\n",
    "noted thanks\n",
    "did u see that email\n",
    "yeah the client is angry\n",
    "what did we do wrong\n",
    "sent the wrong file\n",
    "oops thats bad\n",
    "we need to fix it asap\n",
    "on it right now\n",
    "lunch break?\n",
    "starving lets go\n",
    "subway or tacos\n",
    "tacos definitely\n",
    "hey happy birthday\n",
    "thanks bro\n",
    "doing anything fun\n",
    "dinner with fam then drinks\n",
    "sounds fun enjoy\n",
    "u coming out later\n",
    "maybe if im not tired\n",
    "come on it will be lit\n",
    "ok ill try\n",
    "hey u up\n",
    "cant sleep\n",
    "too much coffee?\n",
    "yeah big mistake\n",
    "read a book it helps\n",
    "boring lol\n",
    "try warm milk\n",
    "gross no way\n",
    "count sheep then\n",
    "haha very funny\n",
    "goodnight\n",
    "night\n",
    "omg did u see her dress\n",
    "yeah stunning\n",
    "expensive tho\n",
    "worth it for the wedding\n",
    "are u bringing a plus one\n",
    "yeah mark is coming\n",
    "cool see u there\n",
    "battery low ttyl\n",
    "charge ur phone\n",
    "i forgot my charger\n",
    "classic u\n",
    "buy one at the store\n",
    "too expensive there\n",
    "rip ur phone then\n",
    "lol bye\n",
    "honey can u buy milk\n",
    "and eggs pls\n",
    "sure anything else\n",
    "maybe bread\n",
    "got it on my way\n",
    "drive carefully raining hard\n",
    "i will\n",
    "hey dude u busy\n",
    "playing fifa why\n",
    "need help moving a couch\n",
    "ugh fine be there in 20\n",
    "buy me lunch after\n",
    "deal\n",
    "my wifi is down again\n",
    "did u restart the router\n",
    "yeah twice\n",
    "call the company\n",
    "hate talking to them\n",
    "do it via the app\n",
    "smart thinking\n",
    "still not working\n",
    "maybe a jagged cable\n",
    "checking now\n",
    "hey where are u guys\n",
    "waiting at the entrance\n",
    "i dont see u\n",
    "near the big statue\n",
    "oh ok walking there now\n",
    "hurry movie starts in 5\n",
    "running lol\n",
    "this weather is crazy\n",
    "ik freezing cold\n",
    "wear a jacket\n",
    "i left it in the car\n",
    "go get it dummy\n",
    "too far\n",
    "suffer then\n",
    "lol thanks for the sympathy\n",
    "wanna go to the gym\n",
    "too lazy today\n",
    "skip leg day?\n",
    "never skip leg day\n",
    "fine lets go\n",
    "pick me up?\n",
    "omw\n",
    "coffee?\n",
    "always\n",
    "starbucks or dunkin\n",
    "dunkin is cheaper\n",
    "starbucks tastes better\n",
    "fine u pay lol\n",
    "ok meet me there\n",
    "brb bathroom\n",
    "k\n",
    "back\n",
    "that was fast\n",
    "lol\n",
    "so what happened next\n",
    "he just walked away\n",
    "rude\n",
    "ikr i was shocked\n",
    "forget him hes a loser\n",
    "yeah u right\n",
    "focus on urself\n",
    "preach\n",
    "going to bed early\n",
    "sick?\n",
    "just exhausted work was hell\n",
    "get some rest\n",
    "thanks gn\n",
    "sweet dreams\n",
    "what is the wifi password\n",
    "its written on the fridge\n",
    "cant read your handwriting\n",
    "its all caps PASS123\n",
    "thanks connected\n",
    "youtube time\n",
    "lower the volume pls\n",
    "sorry headphones on now\n",
    "u guys want anything from the shop\n",
    "ice cream\n",
    "chocolate flavor\n",
    "get me a soda\n",
    "diet or regular\n",
    "regular pls\n",
    "k be back soon\n",
    "door is locked\n",
    "use the key under the mat\n",
    "unsafe place for a key\n",
    "nobody knows but us\n",
    "fair point\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6569a353-3c0b-4ff1-9959-447798dcb1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_r_ban = \"\"\"\n",
    "কিরে কই তুই\n",
    "এইতো রাস্তায় আছি\n",
    "কখন আসবি\n",
    "আর ১০ মিনিট লাগবে\n",
    "তাড়াতাড়ি আয় সবাই অপেক্ষা করতেছে\n",
    "রাস্তায় প্রচুর জ্যাম\n",
    "আচ্ছা সাবধানে আয়\n",
    "তোর জন্য কি খাবার অর্ডার দিব\n",
    "বিরিয়ানি অর্ডার দে\n",
    "কাচ্চি নাকি চিকেন\n",
    "কাচ্চিই ভালো হবে\n",
    "আচ্ছা ঠিক আছে\n",
    "দোস্ত কালকে কি করছিস\n",
    "কিছু না বাসায় থাকব\n",
    "চল ঘুরতে যাই\n",
    "কোথায় যাবি\n",
    "ভাবছি সিনেপ্লেক্সে মুভি দেখব\n",
    "কোন মুভি\n",
    "নতুন যেটা রিলিজ হইছে\n",
    "আচ্ছা টিকেট কি পাবি\n",
    "অনলাইনে চেক কর\n",
    "ঠিক আছে আমি দেখছি\n",
    "আজকে অফিসে আসবি না\n",
    "না শরীরটা ভালো লাগছে না\n",
    "কি হইছে তোর\n",
    "জ্বর আসছে মনে হয়\n",
    "ডাক্তার দেখাইছিস\n",
    "না প্যারাসিটামল খাইছি\n",
    "সাবধানে থাকিস\n",
    "বস জিজ্ঞেস করতেছিল\n",
    "বলো আমি অসুস্থ\n",
    "আচ্ছা আমি ম্যানেজ করে নিব\n",
    "থ্যাংকস দোস্ত\n",
    "আজকে আবহাওয়াটা খুব সুন্দর\n",
    "হ্যাঁ বৃষ্টি হতে পারে\n",
    "ছাতা নিয়ে বের হইছিস\n",
    "না ভুলে গেছি\n",
    "ভিজে যাবি তো\n",
    "রিকশা নিয়ে নিব সমস্যা নেই\n",
    "খেলা দেখছিস কালকে\n",
    "হ্যাঁ সেই ম্যাচ ছিল\n",
    "সাকিব তো ভালো খেলছে\n",
    "শেষের দিকে একটু টেনশন ছিল\n",
    "হ্যাঁ তবে জিতে গেছি এটাই আসল\n",
    "তুই কি অ্যাসাইনমেন্ট জমা দিছিস\n",
    "না এখনো শেষ করতে পারিনি\n",
    "ম্যাম কিন্তু বকা দিবে\n",
    "আজকে রাতে শেষ করব\n",
    "তোর টা আমাকে একটু দিস তো\n",
    "আচ্ছা মেইল করে দিব\n",
    "ধন্যবাদ\n",
    "বাসায় সবাই কেমন আছে\n",
    "সবাই ভালো তোর কি খবর\n",
    "চলছে মোটামুটি\n",
    "চাকরি কেমন চলছে\n",
    "পড়াশোনার চাপ বেশি\n",
    "হতাশ হস না সব ঠিক হয়ে যাবে\n",
    "চেষ্টা করছি\n",
    "দোস্ত ২০০০ টাকা ধার দিতে পারবি\n",
    "আবার টাকা? আগেরটাই তো দিলি না\n",
    "সামনের সপ্তাহে বেতন পাব দিয়ে দিব\n",
    "আচ্ছা বিকাশে পাঠাচ্ছি\n",
    "থ্যাংক ইউ ভাই\n",
    "ঈদে বাড়ি যাবি না\n",
    "হ্যাঁ টিকেট কাটছি\n",
    "কবে যাবি\n",
    "চাঁদ রাতে\n",
    "সাবধানে যাস ভিড় হবে প্রচুর\n",
    "জানি কিন্তু কিছু করার নেই\n",
    "মায়ের সাথে দেখা করা লাগবে\n",
    "হ্যাঁ ফ্যামিলি আগে\n",
    "শোন বাজারে গেলে একটু কল দিস\n",
    "কেন কিছু লাগবে\n",
    "হ্যাঁ কয়েকটা জিনিস আনা লাগত\n",
    "আচ্ছা লিস্ট দে\n",
    "মাছ আর সবজি আনিস\n",
    "আচ্ছা ঠিক আছে\n",
    "এখন রাখি পরে কথা হবে\n",
    "বাই\n",
    "ভালো থাকিস\n",
    "আল্লাহ হাফেজ\n",
    "শুভ সকাল\n",
    "শুভ রাত্রি\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99b8390-a233-4baf-8c87-df3c97670c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_banglish = \"\"\"\n",
    "kire ki obosta tor\n",
    "ei to choltese tor ki obosta\n",
    "kothay tui ekhon\n",
    "ami to basay\n",
    "ajke ber hobi naki\n",
    "na re ajke mood nai\n",
    "keno ki hoise\n",
    "sorir ta valo na re\n",
    "osudh khaisis\n",
    "hmm nissi thik hoye jabe\n",
    "kaal kintu exam pora hoise\n",
    "dhuro kisu pori nai\n",
    "ami to syllabus e dekhi nai\n",
    "fail korbo mone hoy\n",
    "dost amake ektu note gula dis to\n",
    "accha whatsapp e pathaye dibo\n",
    "thik ase thanks mama\n",
    "kire koi tui\n",
    "ei to rastay jam e atke asi\n",
    "koto khon lagbe\n",
    "ar 10 min lagbe\n",
    "taratari ay hunger games shuru hoye jabe\n",
    "food court e naki cineplex e\n",
    "food court e asi\n",
    "ki khabi bol\n",
    "burger naki pizza\n",
    "pizza khai onek din khai na\n",
    "ok order dicchi\n",
    "taka ase to tor kache\n",
    "na re manibag vule gesi\n",
    "sara jibon tui emon e thakbi\n",
    "are taka patha bikash e\n",
    "accha thik ase\n",
    "oi meye ta ke chinish\n",
    "kon ta\n",
    "oi je lal jama pore ache\n",
    "na re chini na notun mone hoy\n",
    "dekh to facebook e pash kina\n",
    "tui shudu meyei dekh\n",
    "ar tui to sadhu baba\n",
    "lol chup kor\n",
    "ajke khela dekhbi na\n",
    "kon match\n",
    "bd vs ind\n",
    "obossoi shakib khelbe to\n",
    "na injury te ache\n",
    "tahole ar dekhe lav nai\n",
    "dhuro eto negative keno tui\n",
    "jitbe inshallah dekhi\n",
    "kire basay jabi kivabe\n",
    "uber daksi\n",
    "vada koto chay\n",
    "300 taka\n",
    "eto beshi keno\n",
    "bristi hoise tai demand beshi\n",
    "riksha nileo parto\n",
    "riksha pabo koi ei rate\n",
    "shabdhan e jas\n",
    "pouchaye call dis\n",
    "ok tata\n",
    "gm\n",
    "gn\n",
    "kmn achos\n",
    "valoi\n",
    "ki koros\n",
    "bose asi\n",
    "bhat khaisis\n",
    "hmm tui?\n",
    "na ekhon o khai nai\n",
    "khaye ne\n",
    "pore khabo\n",
    "movie dekhbi\n",
    "suggest kor kisu\n",
    "3 idiots dekh\n",
    "onek bar dekhsi\n",
    "tahole notun kichu dekh\n",
    "accha thik ase\n",
    "bye\n",
    "ttyl\n",
    "love u dost\n",
    "hate u too lol\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "690e1d4c-c4b2-4421-9b14-b3cdc076210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = document_1 + document_r_ban + document_banglish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abfdd3b4-41cb-4008-9eb7-931e3969981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/md-al-amin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/md-al-\n",
      "[nltk_data]     amin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cce0b78e-9221-4225-a2e2-f95c9995631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(document.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f7cdce3-23cc-49f3-bfe2-d055d7c824d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\"<unk>\": 0}\n",
    "\n",
    "for token in Counter(tokens).keys():\n",
    "    if token not in vocab:\n",
    "        vocab[token] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aa9e577-ff6b-47d6-b267-36775a5b0c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52f9a2fd-97db-4d1c-9234-5db03d772674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Extract sentence from the dataset\n",
    "input_sentence = []\n",
    "input_sentence = document.split(\"\\n\")\n",
    "len(input_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2b2f9-be92-4935-b704-3278e19086bb",
   "metadata": {},
   "source": [
    "## **Tokenization the sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b03dd262-263f-4724-8ff8-272cb69eab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## text to index\n",
    "def text_to_indices(sentence, vocab):\n",
    "    numerical_sentence = []\n",
    "    for token in sentence:\n",
    "        if token in vocab:\n",
    "            numerical_sentence.append(vocab[token])\n",
    "        else:\n",
    "            numerical_sentence.append(vocab[\"<unk>\"])\n",
    "    return numerical_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93fbfdca-fe5b-4e4d-918f-97175e78c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_numerical_sentences = []\n",
    "for sentence in input_sentence:\n",
    "    input_numerical_sentences.append(text_to_indices(word_tokenize(sentence.lower()), vocab))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1843c94c-c711-410e-be76-fe9ce2e8c7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_numerical_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b00d02-c1b5-415e-af64-2d8347c8e134",
   "metadata": {},
   "source": [
    "## **Preparing Trainig Sequence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a393593a-f117-492b-840e-fcb5f6659536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "922"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sequence = []\n",
    "for sentence in input_numerical_sentences:\n",
    "    for i in range(1, len(sentence)):\n",
    "        training_sequence.append(sentence[: i+1])\n",
    "\n",
    "len(training_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1569f9-c45a-473f-97ad-d810d4280264",
   "metadata": {},
   "source": [
    "## **each sentence sequence is different so have to add adding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f3866d3-0e05-4b5b-83b8-ff9e55c5232d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_list = []\n",
    "for seq in training_sequence:\n",
    "    len_list.append(len(seq))\n",
    "\n",
    "max(len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3618f4f9-686e-4b7b-97a8-88fe6742bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_training_sequence = []\n",
    "for sequence in training_sequence:\n",
    "\n",
    "  padded_training_sequence.append([0]*(max(len_list) - len(sequence)) + sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b31c225-781a-464e-95c3-60f19831c17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 2]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_training_sequence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0245b65-8a09-483b-8bc7-df98fdcd907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_training_sequence = torch.tensor(padded_training_sequence, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "563dcc5e-dd8b-455f-857f-8c59308ea9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,  ...,   0,   1,   2],\n",
       "        [  0,   0,   0,  ...,   1,   2,   3],\n",
       "        [  0,   0,   0,  ...,   2,   3,   4],\n",
       "        ...,\n",
       "        [  0,   0,   0,  ...,   0, 250,   3],\n",
       "        [  0,   0,   0,  ..., 250,   3,  84],\n",
       "        [  0,   0,   0,  ...,   3,  84,  44]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_training_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e82a4ad5-cf2e-427c-a542-c0dc8f792362",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_training_sequence[:, :-1]\n",
    "y = padded_training_sequence[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc785e06-a25f-4237-b720-a63e828464a5",
   "metadata": {},
   "source": [
    "## **Our Custom Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b506a6b-9b01-4898-bd40-c13d394be0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.X.shape[0]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaaa2d94-df39-443b-9618-31c42562f2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "922"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = CustomDataset(X, y)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0407afc-11c7-4925-922f-927be409199c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 1]), tensor(2))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6865f091-d25b-4b22-8a95-a7204f51e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37e2ce52-d59c-40bc-a031-ac83bd1659ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, 100)\n",
    "    self.lstm = nn.LSTM(100, 150, batch_first=True)\n",
    "    # self.lstm = nn.LSTM(150, 350, batch_first=True)\n",
    "    self.fc = nn.Linear(150, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    embedded = self.embedding(x)\n",
    "    intermediate_hidden_states, (final_hidden_state, final_cell_state) = self.lstm(embedded)\n",
    "    output = self.fc(final_hidden_state.squeeze(0))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7629e557-0956-45cc-a60e-34278627b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0477726-ef4d-4d43-8fe9-54ec57e86bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a069b59-f33f-4673-bd46-d6583fc2af71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embedding): Embedding(749, 100)\n",
       "  (lstm): LSTM(100, 150, batch_first=True)\n",
       "  (fc): Linear(in_features=150, out_features=749, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b05d3da5-e2da-427e-9e72-9a2813cc0495",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06d0cccd-0e24-4fdc-b4c4-fe7e1b2ea7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 191.7798\n",
      "Epoch: 2, Loss: 180.4471\n",
      "Epoch: 3, Loss: 169.6712\n",
      "Epoch: 4, Loss: 157.8138\n",
      "Epoch: 5, Loss: 144.2824\n",
      "Epoch: 6, Loss: 129.6953\n",
      "Epoch: 7, Loss: 114.8537\n",
      "Epoch: 8, Loss: 101.0559\n",
      "Epoch: 9, Loss: 87.8798\n",
      "Epoch: 10, Loss: 75.4701\n",
      "Epoch: 11, Loss: 64.4774\n",
      "Epoch: 12, Loss: 54.8067\n",
      "Epoch: 13, Loss: 46.5619\n",
      "Epoch: 14, Loss: 39.3970\n",
      "Epoch: 15, Loss: 33.4891\n",
      "Epoch: 16, Loss: 28.6173\n",
      "Epoch: 17, Loss: 24.7426\n",
      "Epoch: 18, Loss: 21.6282\n",
      "Epoch: 19, Loss: 19.1473\n",
      "Epoch: 20, Loss: 17.1547\n",
      "Epoch: 21, Loss: 15.6131\n",
      "Epoch: 22, Loss: 14.3534\n",
      "Epoch: 23, Loss: 13.3892\n",
      "Epoch: 24, Loss: 12.5894\n",
      "Epoch: 25, Loss: 11.8001\n",
      "Epoch: 26, Loss: 11.2309\n",
      "Epoch: 27, Loss: 10.7033\n",
      "Epoch: 28, Loss: 10.3380\n",
      "Epoch: 29, Loss: 10.0189\n",
      "Epoch: 30, Loss: 9.6837\n",
      "Epoch: 31, Loss: 9.4580\n",
      "Epoch: 32, Loss: 9.1423\n",
      "Epoch: 33, Loss: 8.9801\n",
      "Epoch: 34, Loss: 8.7968\n",
      "Epoch: 35, Loss: 8.6592\n",
      "Epoch: 36, Loss: 8.4589\n",
      "Epoch: 37, Loss: 8.3193\n",
      "Epoch: 38, Loss: 8.1869\n",
      "Epoch: 39, Loss: 8.1105\n",
      "Epoch: 40, Loss: 8.0438\n",
      "Epoch: 41, Loss: 7.8386\n",
      "Epoch: 42, Loss: 7.8814\n",
      "Epoch: 43, Loss: 7.8156\n",
      "Epoch: 44, Loss: 7.6537\n",
      "Epoch: 45, Loss: 7.6201\n",
      "Epoch: 46, Loss: 7.5568\n",
      "Epoch: 47, Loss: 7.6169\n",
      "Epoch: 48, Loss: 7.4270\n",
      "Epoch: 49, Loss: 7.4599\n",
      "Epoch: 50, Loss: 7.4834\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  total_loss = 0\n",
    "\n",
    "  for batch_x, batch_y in dataloader:\n",
    "\n",
    "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(batch_x)\n",
    "\n",
    "    loss = criterion(output, batch_y)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "  print(f\"Epoch: {epoch + 1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ab7b4f1-8176-49ce-899a-8599dbeb63cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "\n",
    "def prediction(model, vocab, text):\n",
    "\n",
    "  # tokenize\n",
    "  tokenized_text = word_tokenize(text.lower())\n",
    "\n",
    "  # text -> numerical indices\n",
    "  numerical_text = text_to_indices(tokenized_text, vocab)\n",
    "\n",
    "  # padding\n",
    "  padded_text = torch.tensor([0] * (max(len_list) - len(numerical_text)) + numerical_text, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "  # send to model\n",
    "  padded_text = padded_text.to(device)  \n",
    "  output = model(padded_text)\n",
    "\n",
    "  # predicted index\n",
    "  value, index = torch.max(output, dim=1)\n",
    "\n",
    "  # merge with text\n",
    "  return text + \" \" + list(vocab.keys())[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fde3609-38e7-4a80-9d11-fd7384e6783d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'হতাশ হস না সব ঠিক হয়ে যাবে'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction(model, vocab, \"হতাশ হস না সব ঠিক হয়ে\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38dcb522-5622-438d-b585-a3cc9f172059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'office e jabi na re'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction(model, vocab, \"office e jabi na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aea2280a-0788-4db4-9bb7-00deda6d8442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'taka  ase'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction(model, vocab, \"taka \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7afe2cd0-6b90-42f1-871b-8edc76649158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "হতাশ হস না\n",
      "হতাশ হস না সব\n",
      "হতাশ হস না সব ঠিক\n",
      "হতাশ হস না সব ঠিক হয়ে\n",
      "হতাশ হস না সব ঠিক হয়ে যাবে\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_tokens = 5\n",
    "input_text = \"হতাশ হস\"\n",
    "\n",
    "for i in range(num_tokens):\n",
    "  output_text = prediction(model, vocab, input_text)\n",
    "  print(output_text)\n",
    "  input_text = output_text\n",
    "  time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c06081dd-b1f8-4792-98ec-d22c32ad8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_top_3(model, vocab, idx_to_word, text):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    \n",
    "    # 1. Tokenize and Preprocess\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    numerical_text = text_to_indices(tokenized_text, vocab)\n",
    "    \n",
    "    # 2. Padding (Using your existing logic)\n",
    "    # Ensure we don't exceed max length, or pad if too short\n",
    "    max_len = max(len_list) # From your earlier variable\n",
    "    \n",
    "    current_len = len(numerical_text)\n",
    "    if current_len < max_len:\n",
    "        padding = [0] * (max_len - current_len)\n",
    "        padded_sequence = padding + numerical_text\n",
    "    else:\n",
    "        # If text is longer than max_len, take the last max_len items\n",
    "        padded_sequence = numerical_text[-max_len:]\n",
    "        \n",
    "    tensor_input = torch.tensor(padded_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # 3. Get Model Output\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor_input) # Raw output numbers\n",
    "        \n",
    "        # 4. Apply Softmax to get Probabilities (0% to 100%)\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # 5. Get Top 3 Values and Indices\n",
    "        top_probs, top_indices = torch.topk(probabilities, 3, dim=1)\n",
    "\n",
    "    # 6. Format Output\n",
    "    print(f\"Input: '{text}'\")\n",
    "    print(\"Next Word Predictions:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Loop through the top 3\n",
    "    for i in range(3):\n",
    "        word_idx = top_indices[0][i].item()\n",
    "        prob = top_probs[0][i].item()\n",
    "        word = idx_to_word.get(word_idx, \"<unk>\")\n",
    "        \n",
    "        results.append((word, prob))\n",
    "        print(f\"{i+1}. {word} \\t ({prob*100:.2f}%)\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "42049d7a-24ed-4a60-8c6f-da22a7652178",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idx_to_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example 1: Banglish\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m _ \u001b[38;5;241m=\u001b[39m predict_top_3(model, vocab, \u001b[43midx_to_word\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaka ase to\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Example 2: Bangla\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'idx_to_word' is not defined"
     ]
    }
   ],
   "source": [
    "# Example 1: Banglish\n",
    "_ = predict_top_3(model, vocab, idx_to_word, \"taka ase to\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Example 2: Bangla\n",
    "_ = predict_top_3(model, vocab, idx_to_word, \"হতাশ হস\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Example 3: English\n",
    "_ = predict_top_3(model, vocab, idx_to_word, \"office e jabi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

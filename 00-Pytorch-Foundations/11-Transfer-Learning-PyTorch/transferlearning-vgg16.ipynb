{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98dd3351-cd92-4b1a-b676-8650be712ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d31148e-2da9-4151-a0a4-2faae38d69c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7894b0f71750>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2355ade0-85ba-495b-b270-07bcd6998c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b65a17b-49b4-4613-88e4-4970482a2ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../09-Hyperparameter-Tuning-Bayesian Search/fashion-mnist_train.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9beff80-9476-49b3-9cac-770bd944302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1: ].values\n",
    "y = df.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c79263a5-2786-42dd-ac16-95e53339d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902488c4-1a00-4d0a-a9e2-3b6870bf02d5",
   "metadata": {},
   "source": [
    "## ***Build our own transformations***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7501f1dc-ba8d-44e0-8edd-06ff8ded7df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms\n",
    "\n",
    "custom_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean = [0.485, 0.456, 0.406],\n",
    "            std = [0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55cc8524-73f6-49f1-aa35-5aa34ca1b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, features, labels, transform):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.label = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # resize 28, 28\n",
    "        image = self.features[index].reshape(28, 28)\n",
    "        # change datatype np.uint8\n",
    "        image = image.astype(np.uint8)\n",
    "\n",
    "        # change B&W to collor\n",
    "        image = np.stack([image]*3, axis=-1)\n",
    "        \n",
    "        # convert array to PIL image\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "        # apply transform\n",
    "        image = self.transform(image)\n",
    "        # print(image.shape)\n",
    "\n",
    "        return image, torch.tensor(self.label[index], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "347cfc36-7a7c-4f55-a925-b648f5d1ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(X_train, y_train, transform=custom_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1dd767d0-ec50-45e1-b71d-4e130de58221",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(X_test, y_test, transform=custom_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "166eb5c3-3ee9-4557-8f89-59cef863e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6a9bf9-9875-4cab-88bc-6279f872dcdf",
   "metadata": {},
   "source": [
    "## Fetch the pretrin model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5038fbcf-ce61-45bd-9c00-186f2a496fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/md-al-amin/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/md-al-amin/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "vgg16 = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a103263-975b-48d6-9990-27d0adf9ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Frize the pretrain parameter by required gurd is False\n",
    "\n",
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9778a5e-4099-4542-9aa5-74b3ae7f7d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fc428c8-3d34-4a37-88e2-90bc2ed0e99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03127a02-62d3-4087-a2d9-98cfbe8f2ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.classifier = nn.Sequential(\n",
    "    nn.Linear(25088, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95e97f82-cafd-4b04-a5e2-c01176d911c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7188e08b-6309-4eea-82ca-1751a1bd8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = vgg16.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "823e0a1e-b268-411b-beff-e3968c542df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c4cdc502-8047-439c-b802-8d1d9d8426d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimzer = optim.Adam(vgg16.classifier.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01073711-e874-47f4-9c3d-949220d11a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 , Loss: 0.3670301072448492\n",
      "Epoch: 2 , Loss: 0.21841417237184943\n",
      "Epoch: 3 , Loss: 0.1679442491264393\n",
      "Epoch: 4 , Loss: 0.13248363182352235\n",
      "Epoch: 5 , Loss: 0.10481540832016617\n",
      "Epoch: 6 , Loss: 0.08372377750750941\n",
      "Epoch: 7 , Loss: 0.06608273853835998\n",
      "Epoch: 8 , Loss: 0.05476379886280241\n",
      "Epoch: 9 , Loss: 0.04864088296199528\n",
      "Epoch: 10 , Loss: 0.04174656290608012\n"
     ]
    }
   ],
   "source": [
    "## traing loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_epoch_loss = 0\n",
    "    for batch_features, batch_labels in train_loader:\n",
    "        ## move dataset into the gpu\n",
    "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "        ## forward\n",
    "        output = vgg16(batch_features)\n",
    "\n",
    "        ## loss\n",
    "        loss = criterion(output, batch_labels)\n",
    "\n",
    "        ## backward\n",
    "        optimzer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        ## update the weight\n",
    "        optimzer.step()\n",
    "\n",
    "        total_epoch_loss = total_epoch_loss + loss.item()\n",
    "\n",
    "    avg_loss = total_epoch_loss/len(train_loader)\n",
    "    print(f'Epoch: {epoch + 1} , Loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bfce65dc-2918-4c87-abaa-98b34582fcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9168333333333333\n"
     ]
    }
   ],
   "source": [
    "# evaluation code\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "  for batch_features, batch_labels in test_loader:\n",
    "    batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "\n",
    "    outputs = vgg16(batch_features)\n",
    "    # print(outputs[0])\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    # print(predicted)\n",
    "\n",
    "    total = total + batch_labels.shape[0]\n",
    "    # print((predicted == batch_labels).sum().item())\n",
    "\n",
    "    correct = correct + (predicted == batch_labels).sum().item()\n",
    "    # break\n",
    "\n",
    "print(correct/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a4116-72e2-47e3-8ebd-99f11b17f955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
